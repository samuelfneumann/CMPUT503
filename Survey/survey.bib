@article{farahmand2015classificationbased,
  title={Classification-based Approximate Policy Iteration},
  author={Farahmand, Amir-massoud and Precup, Doina and Barreto, Andr{\'e} MS and Ghavamzadeh, Mohammad},
  journal={IEEE Transactions on Automatic Control},
  year={2015}
}

@inproceedings{lagoudakis2003reinforcement,
  title={Reinforcement Learning as Classification: Leveraging Modern Classifiers},
  author={Lagoudakis, Michail G and Parr, Ronald},
  booktitle={International Conference on Machine Learning},
  year={2003}
}

@inproceedings{steckelmacher2019sample,
  title={Sample-efficient model-free reinforcement learning with off-policy critics},
  author={Steckelmacher, Denis and Plisnier, H{\'e}l{\`e}ne and Roijers, Diederik M and Now{\'e}, Ann},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  year={2019}
}

@article{sun2018dual,
  title={Dual policy iteration},
  author={Sun, Wen and Gordon, Geoffrey J and Boots, Byron and Bagnell, J},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{parisotto2016actor,
  title={Actor-mimic: Deep multitask and transfer reinforcement learning},
  author={Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

@inproceedings{shao2022grac,
  title={Grac: Self-guided and self-regularized actor-critic},
  author={Shao, Lin and You, Yifan and Yan, Mengyuan and Yuan, Shenli and Sun, Qingyun and Bohg, Jeannette},
  booktitle={Conference on Robot Learning},
  year={2022}
}

@inproceedings{simmons2019q,
  title={Q-learning for continuous actions with cross-entropy guided policies},
  author={Simmons-Edler, Riley and Eisner, Ben and Mitchell, Eric and Seung, Sebastian and Lee, Daniel},
  booktitle={RL4RealLife Workshop at the International Conference on Machine Learning},
  year={2019}
}

@article{jaakkola1994convergence,
  title={On the convergence of stochastic iterative dynamic programming algorithms},
  author={Jaakkola, Tommi and Jordan, Michael I and Singh, Satinder P},
  journal={Neural computation},
  volume={6},
  number={6},
  pages={1185--1201},
  year={1994},
  publisher={MIT Press}
}
@inproceedings{obando2021revisiting,
  title={Revisiting Rainbow: Promoting more insightful and inclusive deep reinforcement learning research},
  author={Obando-Ceron, Johan S and Castro, Pablo Samuel},
  booktitle={International Conference on Machine Learning},
  year={2021}
}

@article{ghiassian2020improving,
  title={Improving performance in reinforcement learning by breaking generalization in neural networks},
  author={Ghiassian, Sina and Rafiee, Banafsheh and Lo, Yat Long and White, Adam},
  journal={International Conference on Autonomous Agents and Multiagent Systems},
  year={2020}
}

@inproceedings{pourchot2018cem,
  title={CEM-RL: Combining evolutionary and gradient-based methods for policy search},
  author={Pourchot, Alo{\"\i}s and Sigaud, Olivier},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle={International Conference on Learning Representations},
  year={2014}
}

@inproceedings{melo2007convergence,
  title={Convergence of Q-learning with linear function approximation},
  author={Melo, Francisco S and Ribeiro, M Isabel},
  booktitle={2007 European Control Conference (ECC)},
  pages={2671--2678},
  year={2007},
  organization={IEEE}
}

@article{lazic2021optimization,
  title={Optimization Issues in KL-Constrained Approximate Policy Iteration},
  author={Lazi{\'c}, Nevena and Hao, Botao and Abbasi-Yadkori, Yasin and Schuurmans, Dale and Szepesv{\'a}ri, Csaba},
  journal={arXiv preprint arXiv:2102.06234},
  year={2021}
}

@article{sutton1996generalization,
  title={Generalization in reinforcement learning: Successful examples using sparse coarse coding},
  author={Sutton, Richard S},
  journal={Advances in neural information processing systems},
  pages={1038--1044},
  year={1996},
  publisher={Citeseer}
}

@book{sutton2018reinforcement,
  title={Reinforcement Learning: An Introduction},
  author={Sutton, Richard S. and Barto, Andrew G.},
  year={2018},
  publisher={MIT press}
}

@phdthesis{sutton1984temporal,
author = {Sutton, Richard S.},
title = {Temporal Credit Assignment in Reinforcement Learning},
year = {1984},
school = {University of Massachusetts Amherst}
}

@article{williams1992simple,
  title={Simple Statistical Gradient-following Algorithms for Connectionist Reinforcement Learning},
  author={Williams, Ronald J},
  journal={Machine Learning},
  volume={8},
  number={3-4},
  year={1992},
  publisher={Springer}
}

@inproceedings{kakade2001naturalpg,
  author =       {Kakade, Sham},
  title =        {A Natural Policy Gradient},
  booktitle =    {Advances in Neural Information Processing Systems},
  year =         {2001}
}

@inproceedings{vieillard2020leverage,
	title={Leverage the Average: an Analysis of KL Regularization in Reinforcement Learning},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Vieillard, Nino and Kozuno, Tadashi and Scherrer, Bruno and Pietquin, Olivier and Munos, Remi and Geist, Matthieu},
	year = {2020},
}

@article{tomar2020mirror,
  title={Mirror descent policy optimization},
  author={Tomar, Manan and Shani, Lior and Efroni, Yonathan and Ghavamzadeh, Mohammad},
  journal={arXiv preprint arXiv:2005.09814},
  year={2020}
}

%%%% Entropy and theoretical support for softmax

@inproceedings{shani2019adaptive,
  title={Adaptive trust region policy optimization: Global convergence and faster rates for regularized mdps},
  author={Shani, Lior and Efroni, Yonathan and Mannor, Shie},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2020}
}

@InProceedings{mei_global2020,
	title = {On the Global Convergence Rates of Softmax Policy Gradient Methods},
    author = {Mei, Jincheng and Xiao, Chenjun and Szepesvari, Csaba and Schuurmans, Dale},
    booktitle = {International Conference on Machine Learning},
    year = {2020}
}

@InProceedings{geist2019theory,
  title = 	 {A Theory of Regularized Markov Decision Processes},
  author = 	 {Geist, Matthieu and Scherrer, Bruno and Pietquin, Olivier},
  booktitle = 	 {International Conference on Machine Learning},
  year = 	 {2019}
}

@InProceedings{ahmed2018understanding,
  title = 	 {Understanding the Impact of Entropy on Policy Optimization},
  author = 	 {Ahmed, Zafarali and Le Roux, Nicolas and Norouzi, Mohammad and Schuurmans, Dale},
  booktitle = 	 {International Conference on Machine Learning},
  year = 	 {2019}
}

@article{neu2017unified,
  title={A Unified View of Entropy-regularized Markov Decision Processes},
  author={Neu, Gergely and Jonsson, Anders and G{\'o}mez, Vicen{\c{c}}},
  journal={arXiv:1705.07798},
  year={2017}
}


%%%%% Bias results
@inproceedings{imani2018off,
  title={An Off-policy Policy Gradient Theorem using Emphatic Weightings},
  author={Imani, Ehsan and Graves, Eric and White, Martha},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}

@inproceedings{graves2021offpolicy,
  title={Off-policy Policy Actor-Critic using Emphatic Weightings},
  author={Graves, Eric and Imani, Ehsan and Kumaraswamy, Raksha and White, Martha},
  booktitle={arXiv:2111.08172v1},
  year={2021}
}

@InProceedings{thomas2014bias,
  title = 	 {Bias in Natural Actor-Critic Algorithms},
  author = 	 {Philip Thomas},
  booktitle = 	 {International Conference on Machine Learning},
  year = 	 {2014}
}

@inproceedings{nota2019policy,
  title={Is the Policy Gradient a Gradient?},
  author={Nota, Chris and Thomas, Philip S.},
    booktitle = {International Conference on Autonomous Agents and Multiagent Systems},
    year = {2020}
}

%%% PG Methods
@InProceedings{schulman2015trust,
  title = 	 {Trust Region Policy Optimization},
  author = 	 {John Schulman and Sergey Levine and Pieter Abbeel and Michael Jordan and Philipp Moritz},
  booktitle = 	 {International Conference on Machine Learning},
  year = 	 {2015}
}

@inproceedings{mei2019principled,
  title={On principled entropy exploration in policy optimization},
  author={Mei, Jincheng and Xiao, Chenjun and Huang, Ruitong and Schuurmans, Dale and M{\"u}ller, Martin},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2019}
}

@inproceedings{sutton2000policy,
  title={Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  author={Sutton, Richard S. and McAllester, David A. and Singh, Satinder P. and Mansour, Yishay},
  booktitle={Advances in Neural Information Processing Systems},
  year={1999}
}

@inproceedings{silver2014deterministic,
  title={Deterministic Policy Gradient Algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={International Conference on Machine
Learning},
  year={2014}
}

% DDPG
@inproceedings{lillicrap2015continuous,
  title={Continuous Control with Deep Reinforcement Learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
    booktitle = {International Conference on Learning Representations},
    year = {2016}
}

@inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2008}
}

@inproceedings{konda2000actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay R and Tsitsiklis, John N},
  booktitle={Advances in Neural Information Processing Systems},
  year={2000}
}

@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  year={2018}
}


@article{schulman2017proximal,
  title={Proximal Policy Optimization Algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv:1707.06347},
  year={2017}
}

% SQL
@inproceedings{haarnoja2017reinforcement,
  title={Reinforcement Learning with Deep Energy-based Policies},
  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  year={2017},
}

@article{islam2019entropy,
    title={Entropy Regularization with Discounted Future State Distribution in Policy Gradient Methods},
    author={Riashat Islam and Raihan Seraj and Pierre-Luc Bacon and Doina Precup},
    year={2019},
    journal={arXiv:1912.05104}
}

@article{wang2016sample,
  title={Sample Efficient Actor-Critic with Experience Replay},
  author={Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
  journal={International Conference on Learning Representations},
  year={2017}
}
@article{schulman2015high,
  title={High-dimensional Continuous Control using Generalized Advantage Estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={International Conference on Learning Representations},
  year={2016}
}

@inproceedings{abdolmaleki2018maximum,
    title={Maximum a Posteriori Policy Optimisation},
    author={Abbas Abdolmaleki and Jost Tobias Springenberg and Yuval Tassa and Remi Munos and Nicolas Heess and Martin Riedmiller},
    booktitle={International Conference on Learning Representations},
    year={2018}
}

@inproceedings{vieillard2019deep,
  title={Deep Conservative Policy Iteration},
  author={Vieillard, Nino and Pietquin, Olivier and Geist, Matthieu},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2020}
}

@inproceedings{kober2008policy,
  title={Policy search for motor primitives in robotics},
  author={Kober, Jens and Peters, Jan R},
  booktitle={Advances in Neural Information Processing Systems},
  year={2008}
}

@inproceedings{neumann2011variational,
  title={Variational inference for policy search in changing situations},
  author={Neumann, Gerhard},
  booktitle={International Conference on Machine Learning},
  year={2011}
}

@inproceedings{
  peters2010relative,
  author = {Peters, Jan and M\"{u}lling, Katharina and Alt\"{u}n, Yasemin},
  title = {Relative Entropy Policy Search},
  year = {2010},
  booktitle = {AAAI Conference on Artificial Intelligence},
}

@inproceedings{degris2012model,
  title={Model-Free Reinforcement Learning with Continuous Action in Practice},
  author={Degris, Thomas and Pilarski, Patrick M and Sutton, Richard S},
  booktitle={American Control Conference},
  year={2012}
}

@inproceedings{degris2012offpac,
  author    = {Thomas Degris and
  		Martha White and
		Richard S. Sutton},
  title     = {Off-Policy Actor-Critic},
  booktitle   = {International Conference on Machine Learning},
  year      = {2012}
  }


@inproceedings{gu2016qprop,
  author    = {Gu, Shixiang and
               Lillicrap, Timothy P. and
               Ghahramani, Zoubin and
               Turner, Richard E. and
               Levine, Sergey},
  title     = {Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic},
  booktitle   = {International Conference on Learning Representations},
  year      = {2017}
}

%%%%% API methods
@inproceedings{lazaric2010analysis,
  author =    {Alessandro Lazaric and Mohammad Ghavamzadeh and R{\'e}mi Munos},
  title =     {Analysis of a Classification-based Policy Iteration Algorithm},
  booktitle = {International Conference on Machine Learning},
  year =      2010
}

@inproceedings{chan2021greedification,
  author =    {Alan Chan and Hugo Silva and Sungsu Lim and Tadashi Kozuno and A. Rupam Mahmood and Martha White},
  title =     {Greedification Operators for Policy Optimization: Investigating Forward and Reverse KL Divergences},
  booktitle = {arXiv:2107.08285v1},
  year =      2021
}

@inproceedings{wagner2011reinterpretation,
  title={A Reinterpretation of the Policy Oscillation Phenomenon in Approximate Policy Iteration},
  author={Wagner, Paul},
  booktitle={Advances in Neural Information Processing Systems},
  year={2011}
}

@inproceedings{nachum2017bridging,
  title={Bridging the Gap between Value and Policy Based Reinforcement Learning},
  author={Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{schulman2017equivalence,
  title={Equivalence between Policy Gradients and Soft Q-learning},
  author={Schulman, John and Chen, Xi and Abbeel, Pieter},
  journal={arXiv:1704.06440},
  year={2017}
}

@article{agarwal2021theory,
  title={On the theory of policy gradient methods: Optimality, approximation, and distribution shift},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={98},
  pages={1--76},
  year={2021}
}

@inproceedings{ghosh2020operator,
 author = {Ghosh, Dibya and C. Machado, Marlos and Le Roux, Nicolas},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {An operator view of policy gradient methods},
 year = {2020}
}

@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={International Conference on Machine Learning},
  year={2002}
}

@inproceedings{wagner2013optimistic,
  title={Optimistic Policy Iteration and Natural Actor-Critic: A Unifying View and a Non-optimality Result},
  author={Wagner, Paul},
  booktitle={Advances in Neural Information Processing Systems},
  year={2013}
}

@article{o2016combining,
  title={Combining Policy gradient and Q-learning},
  author={O'Donoghue, Brendan and Munos, Remi and Kavukcuoglu, Koray and Mnih, Volodymyr},
  journal={International Conference on Learning Representations},
  year={2017}
}

%%%% Continus Q learning methods
@article{millan2002continuous,
author={Mill{\'a}n, Jos{\'e} del R. and Posenato, Daniele and Dedieu, Eric},
title={Continuous-Action Q-Learning},
journal={Machine Learning},
year={2002},
volume={49},
number={2}
}

@inproceedings{ryu2019caql,
    title={CAQL: Continuous Action Q-Learning},
    author={Moonkyung Ryu and Yinlam Chow and Ross Anderson and Christian Tjandraatmadja and Craig Boutilier},
    year={2020},
    booktitle={International Conference on Learning Representations}
}

@inproceedings{kalashnikov2018qt,
  author    = {Dmitry Kalashnikov and
               Alex Irpan and
               Peter Pastor and
               Julian Ibarz and
               Alexander Herzog and
               Eric Jang and
               Deirdre Quillen and
               Ethan Holly and
               Mrinal Kalakrishnan and
               Vincent Vanhoucke and
               Sergey Levine},
  title     = {QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation},
  booktitle   = {Conference on Robot Learning},
  year      = {2018}
}

@inproceedings{gu2016naf,
  author    = {Shixiang Gu and
               Timothy P. Lillicrap and
               Ilya Sutskever and
               Sergey Levine},
  title     = {Continuous Deep Q-Learning with Model-based Acceleration},
  booktitle   = {International Conference on Machine Learning},
  year      = {2016}
}

@inproceedings{gaskett1993qlearning,
author = {Gaskett, Chris and Wettergreen, David and Zelinsky, Alexander},
title = {Q-Learning in Continuous State and Action Spaces},
booktitle = {Advanced Topics in Artificial Intelligence},
year = {1999}
}

@inproceedings{amos2016icnn,
  author    = {Amos, Brandon and
               Xu, Lei and
               Kolter, J. Zico},
  title     = {Input Convex Neural Networks},
  year      = {2017},
  booktitle = {International Conference on Machine Learning},
}

%%%%%%%%% CEM citations
@book{rubinstein2004cross,
  title={The cross-entropy method: a unified approach to combinatorial optimization, Monte-Carlo simulation, and machine learning},
  author={Rubinstein, Reuven Y and Kroese, Dirk P},
  volume={133},
  year={2004},
  publisher={Springer}
}

@inproceedings{goschin2013cross,
  title={The Cross-Entropy Method Optimizes for Quantiles},
  author={Goschin, Sergiu and Weinstein, Ari and Littman, Michael},
  booktitle={International Conference on Machine Learning},
  year={2013}
}

@inproceedings{mannor2005cross,
  title={The cross entropy method for classification},
  author={Mannor, Shie and Peleg, Dori and Rubinstein, Reuven},
  booktitle={International Conference on Machine learning},
  year={2005}
  }

  @ARTICLE{szita2006tetris,
author={Istv{\'a}n Szita and Andr{\'a}s L{\"o}rincz},
journal={Neural Computation},
title={Learning Tetris Using the Noisy Cross-Entropy Method},
year={2006},
volume={18},
number={12}
}

@article{rubinstein1999cem,
author={Rubinstein, Reuven},
title={The Cross-Entropy Method for Combinatorial and Continuous Optimization},
journal={Methodology And Computing In Applied Probability},
year={1999},
volume={1},
number={2}
}

@article{hu2012stochastic,
	title={A Stochastic Approximation Framework for a Class of Randomized Optimization Algorithms},
	author={Hu, Jiaqiao and Hu, Ping and Chang, Hyeong Soo},
	journal={IEEE Transactions on Automatic Control},
	volume={57},
	number={1},
	year={2012}
}

@inproceedings{mannor2003cem,
 author = {Mannor, Shie and Rubinstein, Reuven and Gat, Yohai},
 title = {The Cross Entropy Method for Fast Policy Search},
 booktitle = {International Conference on Machine Learning},
 year = {2003}
 }


 @inproceedings{peters2007reinforcement,
 author = {Peters, Jan and Schaal, Stefan},
 title = {Reinforcement Learning by Reward-weighted Regression for Operational Space Control},
 booktitle = {International Conference on Machine Learning},
 year = {2007}
 }

 @article{salimans2017evolution,
  author    = {Salimans, Tim and
               Ho, Jonathan and
               Chen, Xi and
               Sutskever, Ilya},
  title     = {Evolution Strategies as a Scalable Alternative to Reinforcement Learning},
  journal   = {arXiv preprint arXiv:1703.03864},
  year      = {2017}
  }

@article{hansen2003cma,
 author = {Hansen, Nikolaus and M\"{u}ller, Sibylle D. and Koumoutsakos, Petros},
 title = {Reducing the Time Complexity of the Derandomized Evolution Strategy with Covariance Matrix Adaptation (CMA-ES)},
 journal = {Evolutionary Computation},
 volume = {11},
 number = {1},
 year = {2003}
}

@incollection{robbins1985stochastic,
	title={A Stochastic Approximation Method},
	author={Robbins, Herbert and Monro, Sutton},
	booktitle={Herbert Robbins Selected Papers},
	year={1985},
	publisher={Springer}
}

@article{morris1982natural,
	title={Natural Exponential Families with Quadratic Variance Functions},
	author={Morris, Carl N},
	journal={The Annals of Statistics},
	year={1982}
}

@book{borkar2008stochastic,
	title={Stochastic Approximation: A Dynamical Systems Viewpoint},
	author={Borkar, Vivek S.},
	year={2008},
	publisher={Cambridge University Press}
}

@book{kushner2012stochastic,
	title={Stochastic Approximation Methods for Constrained and Unconstrained Systems},
	author={Kushner, Harold J. and Clark, Dean S.},
	year={2012},
	publisher={Springer Science \& Business Media}
}

@article{borkar1997stochastic,
	title={Stochastic Approximation with Two Time Scales},
	author={Borkar, Vivek S},
	journal={Systems \& Control Letters},
	volume={29},
	number={5},
	year={1997}
}

@article{homem2007study,
	title={A Study on the Cross-Entropy Method for Rare-Event Probability Estimation},
	author={Homem-de-Mello, Tito},
	journal={INFORMS Journal on Computing},
	volume={19},
	number={3},
	year={2007}
}

@article{hu2007model,
  title={A Model Reference Adaptive Search Method for Global Optimization},
  author={Hu, Jiaqiao and Fu, Michael C and Marcus, Steven I},
  journal={Operations Research},
  volume={55},
  number={3},
  year={2007}
}

@book{sen2017large,
	title={Large Sample Methods in Statistics (1994): An Introduction with Applications},
	author={Sen, Pranab K and Singer, Julio M},
	year={2017},
	publisher={CRC Press}
}

@article{hoeffding1963probability,
	title={Probability Inequalities for Sums of Bounded Random Variables},
	author={Hoeffding, Wassily},
	journal={Journal of the American Statistical Association},
	volume={58},
	number={301},
	year={1963}
}

@book{rubinstein1993discrete,
	title={Discrete Event Systems: Sensitivity Analysis and Stochastic Optimization by the Score Function Method},
	author={Rubinstein, Reuven Y and Shapiro, Alexander},
	volume={1},
	year={1993},
	publisher={Wiley New York}
}

@book{durrett1991probability,
	title={Probability: Theory and Examples (Wadsworth and Brooks/Cole Statistics/Probability Series)},
	author={Durrett, Richard},
	publisher={Wadsworth \& Brooks/Cole Advanced Books \& Software},
	year={1991}
}

@article{young19minatar,
author = {{Young}, Kenny and {Tian}, Tian},
title = {MinAtar: An Atari-Inspired Testbed for Thorough and Reproducible Reinforcement Learning Experiments},
journal = {arXiv preprint arXiv:1903.03176},
year = "2019"
}

@misc{aigym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}

@article{SACv2,
  author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  title = {Soft Actor-Critic Algorithms and Applications},
  year = {2018},
  journal={arXiv preprint arXiv:1812.05905}
}

